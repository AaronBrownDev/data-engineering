# Data Engineering Learning Journey

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Golang](https://img.shields.io/badge/Go-1.20+-teal.svg)
![Progress](https://img.shields.io/badge/Progress-In%20Progress-orange)

## 🎯 Purpose

This repository documents my structured learning journey in data engineering, tracking my progression from fundamentals to advanced concepts. It serves as both a personal knowledge base and a portfolio of my growing expertise in building data pipelines, creating data catalogs, and implementing data governance solutions.

## 🚀 Learning Goals

I'm working toward mastering these key areas of data engineering:

1. **Data Pipeline Development**: Building robust ETL/ELT pipelines for data integration
2. **Data Catalog Implementation**: Creating systems to organize and document data assets
3. **Large-Scale Data Processing**: Working with technologies that handle high-volume data
4. **Data Governance & Security**: Implementing best practices for data management
5. **Performance Optimization**: Tuning systems for efficient data processing

My immediate goal is to prepare for a Data Management role that involves building solutions covering the entire lifecycle of data: ingestion, integration, development, cataloging, security, and governance.

## 🛠️ Skills Under Development

### Programming Languages
- **Go**: Building efficient, concurrent data processing services
- **Python**: Developing data pipelines and analysis tools
- **SQL**: Writing optimized queries and designing database schemas
- **Java/Scala**: Working with JVM-based big data frameworks (future goal)

### Technologies & Frameworks
- **Databases**: PostgreSQL, Microsoft SQL Server, MongoDB
- **Big Data**: Apache Spark, Hadoop ecosystem
- **Streaming**: Apache Kafka, Apache Airflow
- **Cloud Platforms**: AWS (S3, Redshift, Glue), GCP (BigQuery, Dataflow)
- **Infrastructure as Code**: Terraform, Docker

## 📊 Projects

| Project | Status | Description | Key Technologies |
|---------|--------|-------------|------------------|
| [Basic ETL Pipeline](#) | 🔄 In Progress | Extract data from public APIs, transform it, and load it into SQL database | Python, SQLAlchemy, PostgreSQL |
| [Data Catalog Tool](#) | 📝 Planned | Metadata management system for tracking data sources and transformations | Go, RESTful API, PostgreSQL |
| [Real-time Data Processing](#) | 📝 Planned | Stream processing platform for handling continuous data flows | Kafka, Go, Redis |
| [Data Governance Framework](#) | 📝 Planned | Implementation of data quality checks and access controls | Python, Great Expectations, dbt |
| [Analytics Dashboard](#) | 📝 Planned | Visualization platform for data insights | Python, Dash/Streamlit, PostgreSQL |

## 📈 Progress Tracker

### 2025 Timeline

| Month | Focus Area | Goals |
|-------|------------|-------|
| March | Foundations | Set up learning environment, complete Python/Go refreshers, basic ETL concepts |
| April | Pipeline Development | Build first ETL pipeline, practice SQL optimization, study data modeling |
| May | Data Catalog | Design and implement basic metadata management system in Go |
| June | Streaming Data | Learn Kafka fundamentals, implement real-time processing project |
| July | Project Refinement | Polish projects, improve documentation, prepare for interviews |
| August | Application Prep | Finalize portfolio, interview preparation |

### Weekly Updates

Check the [Updates Log](./learning-path/updates.md) for weekly progress reports.

## 🔄 Repository Structure

```
.
├── learning-path/     # Learning materials, course notes, and resource summaries
├── skills/            # Code examples demonstrating specific technical skills
├── projects/          # Data engineering projects with increasing complexity
└── interview-prep/    # Preparation materials for technical interviews
```

## 🔗 Connect With Me

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/in/aaronbrowndev/)

---

> This journey is a continuous work in progress. I'm committed to daily learning and consistent improvement in the data engineering field.
